{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fdc7366-3834-4322-9f4f-f4f68070798b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Big Data Analytics Praktikum 3\n",
    "\n",
    "**Abfragen mit Apache Spark** \n",
    "\n",
    "Machen Sie sich mit Apache Spark vertraut. Bearbeiten Sie die Aufgaben indem Sie **Spark RDD** entsprechend transformieren. \n",
    "\n",
    "## Arbeitsanweisung\n",
    "Nutzen Sie die markierten Zellen im vorliegenden Notebook `BDA1_A3_Spark.ipynb` für Ihre Lösungen und laden Sie es in Ilias hoch. In den Zellen muss ausführbarer python code vorliegen. Die Ausgabe soll unterhalb der jeweiligen Zellen produziert werden.\n",
    "Liefern Sie auch aussagekräftiges Markdown zu Ihrem Code (Vorgehen, Quellen, etc) ab.\n",
    "\n",
    "**Hinweis**: Verwenden Sie für diese Aufgaben *nicht* Spark SQL und *keine* Dataframes. \n",
    "\n",
    "----\n",
    "\n",
    "## Vorbereitung\n",
    "* Verwenden Sie immer den vorgegeben Spark Master um Inkonsistenzen der python3 Versionen zwischen Worker und Client zu verhindern.\n",
    "* Ändern Sie nicht die SparkContext Konfiguration und beenden Sie bitte den SparkContext nachdem Sie die Bearbeitung beenden, um die Resourcen wieder frei zu geben! (`stop_sc1()`)\n",
    "* Stellen Sie sicher, dass `pyspark` installiert ist (pip install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee9da9a-4bdb-4a12-a6a8-8ae19a6b9dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.9/site-packages (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a92662-3732-4c4f-90ad-4e8a00e6df99",
   "metadata": {},
   "source": [
    "Für diese Aufgabe steht ein HDFS mit dem Namenode `namenode` unter Port `19000` bereit. \n",
    "\n",
    "Sie finden die folgenden Dateien darin:\n",
    "\n",
    "* **`/data/bda1/co2data.tsv`**<br>Datensatz von Messungen verschiedener CO<sub>2</sub>-Sensoren\n",
    "* **`/data/bda1/co2data_pm.tsv`**<br>Datensatz von Messungen verschiedener CO<sub>2</sub>-Sensoren mit Partikelmessung, pm ist die Partikeldichte, npm die Partikelanzahl bestimmter Größen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4db35b-551e-4c73-84e3-db273149a68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:39:48) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pprint import pprint\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(sys.version)  # python3 version\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'ipython3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'\n",
    "def stop_sc1():\n",
    "    \"\"\"stop spark context if exists\"\"\"\n",
    "    try:\n",
    "        sc1.stop()\n",
    "        print('Spark Context stopped')\n",
    "    except Exception as ex1:\n",
    "        print(f'No context stopped: {ex1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68760a31-332d-4d06-b86e-40a70a9f31a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Spark Context erzeugen\n",
    "Die Session mit dem vorgegebenen Spark Master öffnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7494bc27-6e35-40be-bee9-493d94ea3e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No context stopped: name 'sc1' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/20 02:59:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "stop_sc1()\n",
    "\n",
    "# never ever change these lines!\n",
    "config = pyspark.SparkConf().setAll([('spark.executor.memory', '1g'), ('spark.executor.cores', '1'), ('spark.cores.max', '2'), ('spark.driver.memory','1g'), (\"spark.app.name\", os.environ['JUPYTERHUB_CLIENT_ID'])])\n",
    "sc1 = pyspark.SparkContext(master='spark://jupiter.bigdata.fh-aachen.de:17077', conf=config)\n",
    "\n",
    "# Festlegen der Dateipfade\n",
    "file_path_co2data = \"hdfs://namenode:19000/data/bda1/co2data.tsv\"\n",
    "file_path_co2data_pm = \"hdfs://namenode:19000/data/bda1/co2data_pm.tsv\"\n",
    "\n",
    "# Laden der Daten in RDDs\n",
    "co2data = sc1.textFile(file_path_co2data)\n",
    "co2data_pm = sc1.textFile(file_path_co2data_pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9304e7-5a61-4396-a45a-b0a0619f2f10",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Aufgabe 3 a\n",
    "\n",
    "Untersuchen Sie die Sensordaten unter ``hdfs://namenode/data/bda1/*``. Beantworten Sie die folgenden Fragen, indem Sie jeweils geeignete RDD Operationen in Spark ausführen:\n",
    "\n",
    "1. Wieviele Messungen sind der Datenmenge `co2data.tsv` zu finden? Wieviele sind es in `co2data_pm.tsv` ?\n",
    "2. Wie lauten die Attributnamen der Datenmenge `co2data_pm.tsv` ? Geben Sie sie zeilenweise aus.\n",
    "3. Wieviele verschiedene Sensoren (angegeben im Feld _serial_number_) enhält die Datenmenge `co2data.tsv` ?  Beachten Sie den Hinweis unter 3a)\n",
    "4. Wieviele Datenpunkte je Sensor liegen vor in `co2data.tsv` ? Geben Sie sowohl Sensor als auch Anzahl aus und beachten Sie den Hinweis unter 3a)\n",
    "5. Was ist der höchste, und was der niedrigste Temperaturwert in der Datenmenge `co2data_pm.tsv` ? \n",
    "6. Was ist der durchschnittliche CO<sub>2</sub>-Wert je Sensor in der Datenmenge `co2data_pm.tsv` ? Runden Sie gerne auf drei Nachkommastellen und beachten Sie den Hinweis unter 3a)\n",
    "\n",
    "**Hinweis:**<br>Die Serial Number besteht aus drei Komponenten: `s_` als Präfix, die eindeutige MAC-Adresse des Sensors und einer Zahl. Die Eindeutigkeit wird nur über die MAC-Adresse bestimmt. Nutzen Sie die MAC-Adresse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d76957-a4c6-4612-bca3-aae7fb8bacfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nr.3a 1) Zählung von Messungen in den Datensätzen\n",
    "- Quelle count: https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis \n",
    "- Die Count Funktion zählt die Anzahl der Elemente im RDD, somit kann die Anzahl der Messungen gezählt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000bc20a-de6c-4ecc-818b-b3059697f134",
   "metadata": {
    "tags": [
     "bda1_ex3_a",
     "ex3_a1"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================================>                  (10 + 2) / 15]\r"
     ]
    }
   ],
   "source": [
    "# Aufgabe 3a 1\n",
    "\n",
    "#Zeilen zählen\n",
    "#num_rows = co2data.count()\n",
    "#print(f'The total number of rows is {num_rows}')\n",
    "\n",
    "#Überblick über Datensatz(nicht lösung für Aufgabe)\n",
    "#first_five = co2data_pm.take(200)\n",
    "#first_five = co2data.take(200)\n",
    "#for row in first_five:\n",
    "#print(row)\n",
    "    \n",
    "num_measurements_co2data = co2data.count()\n",
    "num_measurements_co2data_pm = co2data_pm.count()\n",
    "\n",
    "print(f\"Anzahl der Messungen in co2data.tsv: {num_measurements_co2data}\")\n",
    "print(f\"Anzahl der Messungen in co2data_pm.tsv: {num_measurements_co2data_pm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb1eef-5f5b-46c7-a550-de86f0a0a2da",
   "metadata": {},
   "source": [
    "## Nr.3a 2) Ausgabe der Attributnamen\n",
    "- Quelle split: https://spark.apache.org/examples.html\n",
    "- Quelle first: https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis\n",
    "- first() gibt erste Element des RDDs zurück(in unserem Fall die Zeile mit den Attributnamen)\n",
    "- split() trennt die Attributnamen am Tabulator als Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32cecc2-e1f2-49e9-abb0-a04db9ba0ee4",
   "metadata": {
    "tags": [
     "bda1_ex3_a",
     "ex3_a2"
    ]
   },
   "outputs": [],
   "source": [
    "# Aufgabe 3a 2\n",
    "attribute_names_co2data_pm = co2data_pm.first()\n",
    "print(\"Attributnamen der Datenmenge co2data_pm.tsv:\")\n",
    "for attribute in attribute_names_co2data_pm.split(\"\\t\"):\n",
    "    print(attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a395cc0-d4a7-4f2c-a103-c1f01624fbf8",
   "metadata": {},
   "source": [
    "## Nr.3a 3) Zählung der verschiedenen Sensoren\n",
    "\n",
    "- Quelle map, distinct, filter: https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis\n",
    "- Entfernen des headers des RDD co2data\n",
    "- Extrahieren der MAC - Adresse aus dem serial_number feld\n",
    "- Nutzung der distinct() Methode um einzigartige MAC-Adressen zu ermitteln\n",
    "- Zählung der Adressen miot count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e5ba0-82a5-4fc2-a138-26e02d7cc1c7",
   "metadata": {
    "tags": [
     "bda1_ex3_a",
     "ex3_a3"
    ]
   },
   "outputs": [],
   "source": [
    "# Aufgabe 3a 3\n",
    "# Header entfernen\n",
    "header = co2data.first() \n",
    "co2data_noheader = co2data.filter(lambda line: line != header)\n",
    "\n",
    "# Durchführung der Mac Adressen Filterung\n",
    "mac_addresses = co2data_noheader.map(lambda x: x.split(\"\\t\")[1].split(\"s_\")[1].split(\"_\")[0])\n",
    "sensors = mac_addresses.distinct().count()\n",
    "\n",
    "print(f\"Anzahl der verschiedenen Sensoren in co2data.tsv: {sensors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38185ac0-9837-4ad0-8dec-7cf8e4f595da",
   "metadata": {},
   "source": [
    "## Nr.3a 4) Zählung der Datenpunkte je Sensor\n",
    "Quellen: collect, reduceBykey: https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis\n",
    "\n",
    "- entfernen des haders des RDD co2data\n",
    "- extrahieren der MAC-Adresse aus dem serial_number Feld\n",
    "- zusammenfassung der Datenpunkte pro MAC-Adresse mit reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591784c3-6a1a-4d96-b5d7-44184082f8ef",
   "metadata": {
    "tags": [
     "bda1_ex3_a",
     "ex3_a4"
    ]
   },
   "outputs": [],
   "source": [
    "# Header entfernen\n",
    "header = co2data.first() \n",
    "co2data_noheader = co2data.filter(lambda line: line != header)\n",
    "\n",
    "sensor_counts = co2data_noheader.map(lambda x: (x.split(\"\\t\")[1].split(\"s_\")[1].split(\"_\")[0], 1))\n",
    "sensor_counts = sensor_counts.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"Anzahl der Datenpunkte je Sensor:\")\n",
    "for sensor, count in sensor_counts.collect():\n",
    "    print(f\"Sensor {sensor}: {count} Datenpunkte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e70998-773a-437a-a731-6ae2cc75d19c",
   "metadata": {},
   "source": [
    "## Nr.3a 5) Suchen des höchsten und niedrigsten CO2- Wertes\n",
    "- Quelle min, max: https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis \n",
    "- mit split werden die Zeilen in Elemente aufgeteilt wo ein Tabulatorzeichen ist und dann sechste Element bzw. an der fünften Stelle ausgewählt\n",
    "- mit strip werden die Anführungszeichen entfernt\n",
    "- mit filter werden die ungültigen null-Werte herausgefiltert \n",
    "- mit map(float) werden die Daten in Fließkommazahlen umgewandelt\n",
    "- mit den min und max Funktionen wird schließlich der höchste und niedrigste Temperaturwert herausgefiltert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43760148-b3ce-42ab-9012-1a8a5c3ce962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe 3a 5\n",
    "\n",
    "# extrahiert den Header\n",
    "header = co2data_pm.first()  \n",
    "\n",
    "# filtert den Header aus den Daten heraus\n",
    "co2data_pm_no_header = co2data_pm.filter(lambda row: row != header)  \n",
    "\n",
    "# Filtern der ungültigen Einträge und konvertieren der gültigen Einträge in float\n",
    "temperatures = co2data_pm_no_header.map(lambda x: x.split(\"\\t\")[5].strip('\"')).filter(lambda x: x != 'null').map(float)\n",
    "\n",
    "min_temperature = temperatures.min()\n",
    "max_temperature = temperatures.max()\n",
    "\n",
    "print(f\"Der niedrigste Temperaturwert in der Datenmenge co2data_pm.tsv: {min_temperature}\")\n",
    "print(f\"Der höchste Temperaturwert in der Datenmenge co2data_pm.tsv: {max_temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72765e6a-0236-451f-89d5-7670dbd2289b",
   "metadata": {},
   "source": [
    "## Nr.3a 6) Berechnung des durchschnittlichen C02-Wertes je Sensor\n",
    "- Quelle  collect, mapValues: https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis\n",
    "- extract serial number teilt string an jedem unterstrich und gibt das zweite element zurück(mac adresse)\n",
    "- valid_sensors_and co2 filtert null werte heraus, erzeugt die form der macadressen, entfernt anführungszeichen\n",
    "- sum_counts berechnet Summe und Anzahl der co2 werte\n",
    "- average_co2 berechnet den durschnittlichen co2 wert für jede serial number indem die summe der co2 Werte durch die Anzahl geteilt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab16b6a-822a-42df-9e73-10715d8b73f6",
   "metadata": {
    "tags": [
     "bda1_ex3_a",
     "ex3_a6"
    ]
   },
   "outputs": [],
   "source": [
    "# Entfernen vom Header\n",
    "header = co2data_pm.first()\n",
    "co2data_pm_no_header = co2data_pm.filter(lambda line: line != header)\n",
    "\n",
    "# Funktion zur Extraktion der Seriennummer\n",
    "def extract_serial_number(serial_number):\n",
    "    return serial_number.split('_')[1]\n",
    "\n",
    "# neues RDD mit gültigen Sensoren und CO2-Werten\n",
    "valid_sensors_and_co2 = co2data_pm_no_header \\\n",
    "    .map(lambda x: (extract_serial_number(x.split(\"\\t\")[3].strip('\"')), x.split(\"\\t\")[4].strip('\"'))) \\\n",
    "    .filter(lambda x: x[1] != 'null')\n",
    "\n",
    "# berechnung der co2- werte in float\n",
    "valid_sensors_and_co2 = valid_sensors_and_co2.mapValues(float)\n",
    "\n",
    "# Berechnung Summe und Anzahl der CO2-Werte für jede Seriennummer\n",
    "sum_counts = valid_sensors_and_co2 \\\n",
    "    .mapValues(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "\n",
    "# Berechnung Durchschnitt für jede Seriennummer\n",
    "average_co2 = sum_counts \\\n",
    "    .mapValues(lambda x: x[0]/x[1]) \\\n",
    "    .collect()\n",
    "\n",
    "# Ausgabe der Ergebnisse\n",
    "for serial, avg in average_co2:\n",
    "    print(f\"Mac adress {serial} has an average CO2 value of {avg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7270679d-cf30-40f2-abad-e2d2c08302b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Aufgabe 3 b\n",
    "\n",
    "Machen Sie Aussagen in Markdown zu den beiden Datenmengen, nachdem Sie sich die Information aus Spark gezogen haben.\n",
    "\n",
    "* Kommen Sensoren (MAC-Adresse in Serial Number) in beiden Datenmengen vor? Wenn ja, welche?\n",
    "* Anhand welchen Attributs können Sie auf vorhandene Werte in den Attributen `pm*` und `npm*` filtern? Geben Sie ein Beispiel.\n",
    "* Enthält eine Datenmenge \"fehlerhafte\" CO<sub>2</sub>-Messungen? Wenn ja, wieviele Messungen sind betroffen? Fehlerhaft ist ein Wert `null`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaed2e0-9851-4983-abe0-d10fca199f6c",
   "metadata": {},
   "source": [
    "## Nr.3b 1) Ausgabe der gemeinsamen Sensoren aus beiden Datensätzen\n",
    "-Gemeinsame Sensoren:\n",
    "Sensor: e8db84c5f33d\n",
    "Sensor: 8caab57c89c3\n",
    "Sensor: a848fac03782\n",
    "Sensor: 8caab57c9751\n",
    "Sensor: 8caab57d01da\n",
    "Sensor: e8db84c62ab4\n",
    "Sensor: 8caab57d0593\n",
    "Sensor: 3c6105d45469\n",
    "Sensor: a848fac0a9df\n",
    "Sensor: 3c6105d381e8\n",
    "Sensor: 308398824a5e\n",
    "Sensor: a848fac03ff8\n",
    "Sensor: 3c6105d36eb0\n",
    "Sensor: 3c6105d3908f\n",
    "Sensor: 40915150b550\n",
    "Sensor: 8caab57ccaaf\n",
    "Sensor: d8bfc014724e\n",
    "Sensor: e8db84c5f771\n",
    "Sensor: 308398a2f790\n",
    "Sensor: 308398805c0b\n",
    "Sensor: 3c6105d414f3\n",
    "Sensor: ac0bfbd85271\n",
    "Sensor: 4091514f0284\n",
    "Sensor: 308398b552c4\n",
    "Sensor: 308398a2fb52\n",
    "Sensor: ac0bfbd71044\n",
    "Sensor: 10521c01cf19\n",
    "Sensor: 3c6105d4188d\n",
    "Sensor: 30839882fcc8\n",
    "Sensor: 8caab57cc813\n",
    "Sensor: ac0bfbd6547d\n",
    "Sensor: 308398b54e4c\n",
    "Sensor: 308398b620a0\n",
    "Sensor: 8caab57cc961\n",
    "Sensor: e8db84c5f33d\"\n",
    "Sensor: 10521c0202ab\n",
    "Sensor: 8caab57c3e19\n",
    "Sensor: 8caab57cbb52\n",
    "Sensor: 3c6105d467fd\n",
    "Sensor: 308398a26607\n",
    "Sensor: 8caab57cfb10\n",
    "Sensor: 308398b5b69c\n",
    "Sensor: 308398b60c74\n",
    "Sensor: 8caab57cafa8\n",
    "Sensor: 3c6105cffb3b\n",
    "Sensor: 3c6105d3abae\n",
    "Sensor: 308398b6157d\n",
    "Sensor: 308398a2ddcb\n",
    "Sensor: 308398a2a1f6\n",
    "Sensor: 8caab57a6dd9\n",
    "Sensor: 308398a2000e\n",
    "Sensor: 8caab57b0e22\n",
    "Sensor: ac0bfbd857fb\n",
    "Sensor: 3c6105d34ddc\n",
    "Sensor: 308398a2a8b6\n",
    "Sensor: 308398b595c0\n",
    "Sensor: 3c6105d389cc\n",
    "Sensor: e8db84c60450\n",
    "Sensor: 308398b61fd3\n",
    "Sensor: e8db84c5fc6a\n",
    "Sensor: 30839882dbce\n",
    "Sensor: 8caab57a6dd9\"\n",
    "Sensor: ac0bfbd64321\n",
    "Sensor: d8bfc0147061\n",
    "Sensor: 308398b651ee\n",
    "Sensor: 308398a2a0e4\n",
    "Sensor: 3c6105d49d8d\n",
    "Sensor: 3083988004ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033579b2-cb33-43f3-992f-024270b52baa",
   "metadata": {
    "tags": [
     "bda1_ex2_a",
     "ex2_a2"
    ]
   },
   "outputs": [],
   "source": [
    "#header co2data entfernen\n",
    "header1 = co2data.first() \n",
    "co2data_noheader = co2data.filter(lambda line: line != header1)\n",
    "\n",
    "#header co2data_pm entfernen\n",
    "header2 = co2data_pm.first() \n",
    "co2data_noheader_pm = co2data_pm.filter(lambda line: line != header2)\n",
    "\n",
    "# Sensoren in co2data zählen\n",
    "sensor_counts_co2data = co2data_noheader.map(lambda x: (x.split(\"\\t\")[1].split(\"s_\")[1].split(\"_\")[0], 1))\n",
    "sensor_counts_co2data = sensor_counts_co2data.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Sensoren in co2adata_pm zählen\n",
    "sensor_counts_co2data_pm = co2data_noheader_pm.map(lambda x: (x.split(\"\\t\")[3].split(\"s_\")[1].split(\"_\")[0], 1))\n",
    "sensor_counts_co2data_pm = sensor_counts_co2data_pm.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Mergen der beiden RDDs\n",
    "combined_sensor_counts = sensor_counts_co2data.union(sensor_counts_co2data_pm)\n",
    "\n",
    "# Reduzieren der vereinigten RDDs und Filtern der Sensoren, die in beiden Datensätzen vorhanden sind\n",
    "common_sensors = combined_sensor_counts.reduceByKey(lambda a, b: a + b).filter(lambda x: x[1] > 1)\n",
    "\n",
    "# Ausgeben der gemeinsamen Sensoren\n",
    "common_sensors_list = common_sensors.keys().collect()\n",
    "\n",
    "print(\"Gemeinsame Sensoren:\")\n",
    "for sensor in common_sensors_list:\n",
    "    print(f\"Sensor: {sensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d1813-30dd-4d25-9876-8a699e8811c8",
   "metadata": {},
   "source": [
    "## Nr.3b 2.) Filtern nach pm und npm Werten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01c58f-7f79-42a3-be81-b7d92791de5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#header co2data_pm entfernen\n",
    "header = co2data_pm.first() \n",
    "co2data_noheader_pm = co2data_pm.filter(lambda line: line != header)\n",
    "\n",
    "# Zeilen filtern, keine null-werte in der pm1-Spalte haben\n",
    "non_null_pm1 = co2data_noheader_pm.filter(lambda x: x.split(\"\\t\")[7] != \"null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2295a-cfb5-40f7-897f-1c699ef2ab21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nr.3b 3.) Zählung fehlerhafter Co2-Messungen\n",
    "- Die Ergebnisse zeigen, dass es in dem co2_data Datensatz keinen einzigen co2 'null' Wert gibt\n",
    "- In dem co2_data_pm Datensatz wiederum 9 fehlerhafte Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23817c08-9f86-47c6-bfa6-f9c1cc4d64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtern der Daten, in denen co2_ppm 'null' ist\n",
    "wrong_co2_measurements = co2data.map(lambda x: x.split(\"\\t\")).filter(lambda x: x[3]=='null')\n",
    "wrong_co2_measurements2 = co2data_pm.map(lambda x: x.split(\"\\t\")).filter(lambda x: x[4]=='null')\n",
    "\n",
    "# Zählung der fehlerhaften Messungen\n",
    "num_wrong_measurements = wrong_co2_measurements.count()\n",
    "num_wrong_measurements2 = wrong_co2_measurements2.count()\n",
    "\n",
    "# Ausgabe Anzahl fehlerhafter Messungen\n",
    "print(num_wrong_measurements)\n",
    "print(num_wrong_measurements2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499f787-05a8-49ab-9b7c-95302e4afaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_sc1()  # always exit your spark context after work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d7d8a-b8f7-41f8-b083-21d0da9f79ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nützliche Links\n",
    "* https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "* https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis\n",
    "* https://spark.apache.org/examples.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
